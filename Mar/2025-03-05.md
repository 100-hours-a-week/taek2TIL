## 날짜: 2025-03-05

### 스크럼
1. 어제 한 일: 1. 필기 잘 하기, 2. 미니퀘스트(model architecture, VGG)
2. 오늘 할 일: 1. 수업듣기, 2. 과제 끝내기, 3. 코테 공부하기

### 새로 배운 내용
#### 조기 중단
과적합 방지, 훈련 시간 절약, 최적의 모델을 선택했다는 방증

#### 하이퍼파라미터 튜닝
- 학습 속도 : 가중치 업데이트의 크기를 결정하는 값, 0.01, 0.001, 0.0001
- 배치 크기 : 한 번에 학습할 데이터 샘플의 수, 32, 64, 128
- 에포크 : 전체 데이터셋을 몇 번 반복하여 학습할지, 10, 50, 100
- 옵티마이저 : 모델 학습 과정을 최적화하는 알고리즘, SGD, Adam, RMSprop
- 모멘텀 : SGD와 같은 옵티마이저에서 기울기 업데이트에 대한 관성을 추가하여 학습 가속화, 안정화, 0.9, 0.95, 0.99
- 드롭아웃 비율 : 학습 과정에서 무작위로 뉴런을 죽이는 비율, 0.2, 0.5
- 정규화 파라미터 : 과적합을 방지하기 위해 가중치에 패널티를 부여하는 값, 0.01, 0.001
- 학습률 감쇠 : 학습이 진행됨에 따라 학습률을 감소시키는 방법, 0.1, 0.01, 0.001
- 활성화 함수 : 각 뉴런의 출력값을 결정하는 함수, ReLU, Sigmoid, Tanh
- 초기화 방법 : 가중치 초기화 방법, He, Glorot, Random

### 오늘의 회고
> 오늘도 고생했다

### 참고 자료 및 링크
- 